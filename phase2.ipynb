{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\hari files\\project\\wm_project\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from googletrans import Translator\n",
    "import copy\n",
    "import math\n",
    "from fuzzywuzzy import fuzz\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tamil_stop_words = []\n",
    "with open(\"../Assets/TamilStopWords.txt\", encoding='utf-8') as file:\n",
    "    for line in file.readlines():\n",
    "        tamil_stop_words.append(line.strip())\n",
    "\n",
    "len(tamil_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_with_root = pd.read_csv(r\"D:\\hari files\\project\\wm_project\\Assets\\MostusedRootwords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85256"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tamil_nouns = []\n",
    "with open(\"../Assets/all-tamil-nouns.txt\", encoding='utf-8') as file:\n",
    "    for line in file.readlines():\n",
    "        tamil_nouns.append(line.strip())\n",
    "\n",
    "len(tamil_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1461494"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tamil_verbs = []\n",
    "with open(\"../Assets/Simple-verbs-01022021.txt\", encoding='utf-8') as file:\n",
    "    for line in file.readlines():\n",
    "        tamil_verbs.append(line.strip())\n",
    "\n",
    "len(tamil_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"D:\\hari files\\project\\wm_project\\Assets\\eng_synonyms.json\\eng_synonyms.json\", 'r') as file:\n",
    "    synonums = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>efficient</td>\n",
       "      <td>incompetent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dovish</td>\n",
       "      <td>hawkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>imaginary</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lateral</td>\n",
       "      <td>bottom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>invalid</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word1        word2\n",
       "0  efficient  incompetent\n",
       "1     dovish      hawkish\n",
       "2  imaginary         real\n",
       "3    lateral       bottom\n",
       "4    invalid        valid"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opposites = pd.read_csv(r\"D:\\hari files\\project\\wm_project\\Assets\\opposites.csv\")\n",
    "opposites = opposites.drop([\"Unnamed: 2\"], axis = 1)\n",
    "opposites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "model_tamil = fasttext.load_model(\"D:\\\\hari files\\\\project\\\\wm_project\\\\Assets\\\\pretrained_word_to_vector\\\\cc.ta.300.bin\\\\cc.ta.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_root(w):\n",
    "    for i in range(words_with_root.shape[0]):\n",
    "        derivatives = words_with_root.iloc[i, 2].split(\",\")\n",
    "        if w in derivatives:\n",
    "            if w.endswith(\"ல்லை\"):\n",
    "                return (w,)\n",
    "            return (words_with_root.iloc[i, 0],)\n",
    "    return (w,)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(_sentences):\n",
    "    sentences = []\n",
    "    for _sentence in _sentences:\n",
    "        words = []\n",
    "        for word in _sentence.split():\n",
    "            if word not in tamil_stop_words:\n",
    "                words.append(word)\n",
    "        sentences.append(\" \".join(word for word in words))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_roots(_sentences):\n",
    "    sentences = []\n",
    "    for _sentence in _sentences:\n",
    "        roots = []\n",
    "        for _word in _sentence.split():\n",
    "            root = find_root(_word)\n",
    "            for r in root:\n",
    "                roots.append(r)\n",
    "        sentences.append(roots)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dict_word(word):\n",
    "    for i in range(words_with_root.shape[0]):\n",
    "        if word == words_with_root.iloc[i, 0] or word in words_with_root.iloc[i, 2].split(\",\"):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proper_nouns(sentences):\n",
    "    nouns = []\n",
    "    for sentence in sentences:\n",
    "        sentence_dup = []\n",
    "        for word in sentence:\n",
    "            if not is_dict_word(word) and word not in tamil_verbs:\n",
    "                sentence_dup.append(word)\n",
    "        nouns.append(sentence_dup)\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vectors(sentences):\n",
    "    sentence_vectors = []\n",
    "    for sentence in sentences:\n",
    "        if sentence == []:\n",
    "            sentence_vectors.append(None)\n",
    "            continue\n",
    "        \n",
    "        sentence_vector = []\n",
    "        for word in sentence.split():\n",
    "            word_vector = model_tamil.get_word_vector(word)\n",
    "            sentence_vector.append(word_vector)\n",
    "        sentence_vectors.append(np.mean(sentence_vector, axis = 0))\n",
    "    return sentence_vectors\n",
    "\n",
    "def cosine_similarity(_vec1, _vec2):\n",
    "    return np.dot(_vec1, _vec2) / (np.linalg.norm(_vec1) * np.linalg.norm(_vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_tamil_to_english(word):\n",
    "    translator = Translator()\n",
    "    translation = translator.translate(word, src='ta', dest='en')\n",
    "    return translation.text\n",
    "\n",
    "def find_synonym(w):\n",
    "    for i in range(opposites.shape[0]):\n",
    "        if(opposites.iloc[i, 0] == w):\n",
    "            return w\n",
    "    \n",
    "    if w not in synonums:\n",
    "        return w\n",
    "    \n",
    "    for synonum in synonums[w]:\n",
    "        for i in range(opposites.shape[0]):\n",
    "            if(opposites.iloc[i, 0] == synonum):\n",
    "                return synonum\n",
    "    return w\n",
    "\n",
    "def is_opposite(_word1, _word2):\n",
    "    flag1 = False\n",
    "    flag2 = False\n",
    "\n",
    "    if(_word1.endswith(\"ல்லை\") or _word2.endswith(\"ல்லை\")):\n",
    "        if(_word1.endswith(\"ல்லை\") and _word2[:-2] in _word1[:-6]):\n",
    "            return True\n",
    "        if(_word2.endswith(\"ல்லை\") and _word1[:-2]in _word2[:-6]):\n",
    "            return True\n",
    "\n",
    "    print(_word1, _word2)\n",
    "    _word1 = translate_tamil_to_english(_word1)\n",
    "    _word2 = translate_tamil_to_english(_word2)\n",
    "    w1 = find_synonym(_word1.lower())\n",
    "    w2 = find_synonym(_word2.lower())\n",
    "\n",
    "    for i in range(opposites.shape[0]):   \n",
    "        if(opposites.iloc[i, 0] == w1):\n",
    "            flag1 = True\n",
    "        if(opposites.iloc[i, 0] == w2):\n",
    "            flag2 = True\n",
    "\n",
    "    if(flag1 and flag2):\n",
    "        for i in range(opposites.shape[0]):\n",
    "            if(\n",
    "                (opposites.iloc[i, 0] in w1) and\n",
    "                (opposites.iloc[i, 1] in w2)\n",
    "            ):\n",
    "                return True\n",
    "        return False\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_word_if_illai(input_list):\n",
    "    new_list = copy.deepcopy(input_list)\n",
    "    for j, inner_list in enumerate(input_list, 0):\n",
    "        for i, word in enumerate(inner_list, 0):\n",
    "            if word == 'இல்லை' and new_list[j][i - 1] is not None:\n",
    "                new_list[j][i - 1] = inner_list[i - 1][:-1] + 'ில்லை'\n",
    "                new_list[j].remove(word)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    sentences = []\n",
    "    for sentence in text:\n",
    "        pattern = r'[^\\s\\u0B80-\\u0BFF]'\n",
    "        clean_text = re.sub(pattern, '', sentence)\n",
    "        sentences.append(clean_text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = [\"முதல் முறையாக ஆர்சிபி ஐபிஎல் கோப்பை வென்றது\"]\n",
    "# sentences = [\n",
    "#     \"ஆர்சிபி ஐபிஎல் கோப்பை வென்றதில்லை\",\n",
    "#     \"இனி ராயல் சேலஞ்சர்ஸ் பெங்களூர் இல்லை ஆர்சிபி பெயர் ஜெர்ஸி அதிரடி மாற்றம்\",\n",
    "#     \"ஆர்சிபி தொடர் தோல்விக்கு பின் தோல்வி\",\n",
    "# ]\n",
    "\n",
    "# query = [\"இந்தியா பிரதமர் நரேந்திரா மோடி ஊழல் காரணமாக கைது\"]\n",
    "# sentences = [\n",
    "#     \"பிரதமர் நரேந்திரா மோடி கருபுப்பணம் வழக்கில் சிக்கினார்\",\n",
    "#     \"இந்தியா பிரதமர் இன்று காலை சிறை சென்றார்\",\n",
    "#     \"பாஜக கட்சி மந்திரிகளுக்கு ஐந்து வருடம் சிறை தண்டனை\",\n",
    "# ]\n",
    "\n",
    "# query = [\"இது வரை கொரோனா வைரஸ் காரணமாக எந்த உயிரிழப்பும் இல்லை\"]\n",
    "# sentences = [\n",
    "#                 \"ஆபத்துடன் விளையாடும் சீன ஆய்வாளர்கள் உயிரிழப்பு நிச்சயம் பகீர் கிளப்பும் புது வகை கொரோனா\",\n",
    "#                 \"கொரோனா வைரஸ் உங்கள் ஊரில் ஊரடங்கு நீடிக்கப்படுகிறதா\",\n",
    "#                 \"கொரோனா தொற்று காரணமாக இன்று ஒரே நாளில் நான்கு பேர் உயிரிழப்பு\",\n",
    "#         ]\n",
    "\n",
    "# query = [\"முகேஷ் அம்பானி ஒரு பெண்ணுடன் புதிய திட்டத்திற்கு ஒப்புதல் அளித்துள்ளார்\"]\n",
    "# sentences = [\n",
    "#                 \"இந்திய மக்களுக்கான புதிய முதலீட்டுத் திட்டத்தை அறிவித்த முகேஷ் அம்பானி\",\n",
    "#                 \"முகேஷ் அம்பானி, லைலா ராவ்ஸ் ஃபண்டில் முதலீடு செய்ய மக்களைக் கேட்டுக் கொண்டா\",\n",
    "#         ]\n",
    "\n",
    "query = [\"சந்திரயான் - 3 தென் துருவத்தில் தரையிறங்கியது\"]\n",
    "sentences = [\n",
    "                \"தென் துருவத்தில் தரையிறங்கும் இந்தியா\",\n",
    "                \"சந்திரயான் - 3 நிலவின் தென் துருவத்தில் ஆக்சிஜன் கண்டுபிடிப்பு\",\n",
    "                \"சந்திரயான்- 3 திட்டம் வெற்றி நிலவின் தென் துருவத்தில் கால்பதித்த முதல் நாடு இந்தியா\",\n",
    "                \"அடுத்த வாரம் சந்திரயான் - 3 தென் துருவத்தில் தரையிறங்கத் தயாராகி வருகிறது\"\n",
    "        ]\n",
    "\n",
    "# query = [\"நடிகர் கமல் ஹாசன் இன்று ஆஸ்கார் விருது பெற்றார்\"]\n",
    "# sentences = [\n",
    "#     \"நடிகர் ரஜினிகாந்த் 2003 இன்று ஆஸ்கார் விருது பெற்றார்\",\n",
    "#     \"ஆர்ஆர்ஆர் திரைபடத்திற்கு இந்த வருடம் ஆஸ்கார்\",\n",
    "#     \"தென் இந்தியாவில் இருந்து இரண்டு நடிகர்கள் ஆஸ்கார் விருது பெற்றணர்\"\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['சந்திரயான்   தென் துருவத்தில் தரையிறங்கியது']\n",
      "['தென் துருவத்தில் தரையிறங்கும் இந்தியா', 'சந்திரயான்   நிலவின் தென் துருவத்தில் ஆக்சிஜன் கண்டுபிடிப்பு', 'சந்திரயான்  திட்டம் வெற்றி நிலவின் தென் துருவத்தில் கால்பதித்த முதல் நாடு இந்தியா', 'அடுத்த வாரம் சந்திரயான்   தென் துருவத்தில் தரையிறங்கத் தயாராகி வருகிறது']\n"
     ]
    }
   ],
   "source": [
    "query = remove_special_characters(query)\n",
    "sentences = remove_special_characters(sentences)\n",
    "print(query)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['சந்திரயான் தென் துருவத்தில் தரையிறங்கியது']\n",
      "['தென் துருவத்தில் தரையிறங்கும் இந்தியா', 'சந்திரயான் நிலவின் தென் துருவத்தில் ஆக்சிஜன் கண்டுபிடிப்பு', 'சந்திரயான் திட்டம் வெற்றி நிலவின் தென் துருவத்தில் கால்பதித்த நாடு இந்தியா', 'வாரம் சந்திரயான் தென் துருவத்தில் தரையிறங்கத் தயாராகி வருகிறது']\n"
     ]
    }
   ],
   "source": [
    "stop_free_query = remove_stop_words(query)\n",
    "stop_free_sentences = remove_stop_words(sentences)\n",
    "print(stop_free_query)\n",
    "print(stop_free_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['சந்திரயான்', 'தென்', 'துருவம்', 'தரையிறங்கு']]\n",
      "[['தென்', 'துருவம்', 'தரையிறங்கு', 'இந்தியா'], ['சந்திரயான்', 'நிலவு', 'தென்', 'துருவம்', 'ஆக்சிஜன்', 'கண்டுபிடிப்பு'], ['சந்திரயான்', 'திட்டம்', 'வெற்றி', 'நிலவு', 'தென்', 'துருவம்', 'கால்நடை', 'நாடு', 'இந்தியா'], ['வாரம்', 'சந்திரயான்', 'தென்', 'துருவம்', 'தரையிறங்கு', 'தயாராகி', 'வரு']]\n"
     ]
    }
   ],
   "source": [
    "stop_free_query_roots = convert_to_roots(stop_free_query)\n",
    "stop_free_sentences_roots = convert_to_roots(stop_free_sentences)\n",
    "print(stop_free_query_roots)\n",
    "print(stop_free_sentences_roots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['சந்திரயான்', 'தென்', 'துருவம்', 'தரையிறங்கு']]\n",
      "[['தென்', 'துருவம்', 'தரையிறங்கு', 'இந்தியா'], ['சந்திரயான்', 'நிலவு', 'தென்', 'துருவம்', 'ஆக்சிஜன்', 'கண்டுபிடிப்பு'], ['சந்திரயான்', 'திட்டம்', 'வெற்றி', 'நிலவு', 'தென்', 'துருவம்', 'கால்நடை', 'நாடு', 'இந்தியா'], ['வாரம்', 'சந்திரயான்', 'தென்', 'துருவம்', 'தரையிறங்கு', 'தயாராகி', 'வரு']]\n"
     ]
    }
   ],
   "source": [
    "stop_free_query_roots_joined = join_word_if_illai(stop_free_query_roots)\n",
    "stop_free_sentences_roots_joined = join_word_if_illai(stop_free_sentences_roots)\n",
    "print(stop_free_query_roots_joined)\n",
    "print(stop_free_sentences_roots_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['சந்திரயான்']]\n",
      "[[], ['சந்திரயான்', 'ஆக்சிஜன்'], ['சந்திரயான்'], ['சந்திரயான்', 'தயாராகி']]\n"
     ]
    }
   ],
   "source": [
    "query_proper_nouns = get_proper_nouns(stop_free_query_roots_joined)\n",
    "sentences_proper_nouns = get_proper_nouns(stop_free_sentences_roots_joined)\n",
    "print(query_proper_nouns)\n",
    "print(sentences_proper_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], ['ஆக்சிஜன்'], [], ['தயாராகி']]\n",
      "[[]]\n"
     ]
    }
   ],
   "source": [
    "query_proper_nouns_without_common_items = []\n",
    "sentences_proper_nouns_without_common_items = []\n",
    "\n",
    "for sentence in sentences_proper_nouns:\n",
    "    sentences_proper_nouns_without_common_items.append(\n",
    "        list(set(sentence) - set(query_proper_nouns[0]))\n",
    "    )\n",
    "\n",
    "for query in query_proper_nouns:\n",
    "    qset = set(query)\n",
    "    for sentence in sentences_proper_nouns:\n",
    "        qset = qset - set(sentence)\n",
    "    query_proper_nouns_without_common_items.append(list(qset))\n",
    "\n",
    "print(sentences_proper_nouns_without_common_items)\n",
    "print(query_proper_nouns_without_common_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_proper_nouns_merged = [\" \".join(query) for query in query_proper_nouns_without_common_items]\n",
    "sentences_proper_nouns_merged =  [\" \".join(sentence) for sentence in sentences_proper_nouns_without_common_items]\n",
    "\n",
    "# query_vector = get_sentence_vectors(query_proper_nouns_merged)\n",
    "# sentences_vector = get_sentence_vectors(sentences_proper_nouns_merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.5, 1.0, 0.5]\n"
     ]
    }
   ],
   "source": [
    "proper_noun_scores = []\n",
    "for vector in sentences_proper_nouns_merged:\n",
    "    score = fuzz.ratio(vector, query_proper_nouns_merged[0])\n",
    "    proper_noun_scores.append(score * 0.01 or 0.5)\n",
    "\n",
    "print(proper_noun_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['துருவம்', 'தென்', 'தரையிறங்கு']]\n",
      "[['துருவம்', 'இந்தியா', 'தென்', 'தரையிறங்கு'], ['நிலவு', 'தென்', 'துருவம்', 'கண்டுபிடிப்பு'], ['தென்', 'வெற்றி', 'நாடு', 'நிலவு', 'துருவம்', 'இந்தியா', 'கால்நடை', 'திட்டம்'], ['தென்', 'தரையிறங்கு', 'வரு', 'துருவம்', 'வாரம்']]\n"
     ]
    }
   ],
   "source": [
    "query_without_proper_nouns = []\n",
    "sentences_without_proper_nouns = []\n",
    "for i in range(len(stop_free_sentences_roots_joined)):\n",
    "    sentences_without_proper_nouns.append(list(set(stop_free_sentences_roots_joined[i]) - set(sentences_proper_nouns[i])))\n",
    "query_without_proper_nouns.append(list(set(stop_free_query_roots_joined[0]) - set(query_proper_nouns[0])))\n",
    "\n",
    "print(query_without_proper_nouns)\n",
    "print(sentences_without_proper_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "துருவம் துருவம்\n",
      "துருவம் தென்\n",
      "துருவம் தரையிறங்கு\n",
      "இந்தியா துருவம்\n",
      "இந்தியா தென்\n",
      "இந்தியா தரையிறங்கு\n",
      "தென் துருவம்\n",
      "தென் தென்\n",
      "தென் தரையிறங்கு\n",
      "தரையிறங்கு துருவம்\n",
      "தரையிறங்கு தென்\n",
      "தரையிறங்கு தரையிறங்கு\n",
      "நிலவு துருவம்\n",
      "நிலவு தென்\n",
      "நிலவு தரையிறங்கு\n",
      "தென் துருவம்\n",
      "தென் தென்\n",
      "தென் தரையிறங்கு\n",
      "துருவம் துருவம்\n",
      "துருவம் தென்\n",
      "துருவம் தரையிறங்கு\n",
      "கண்டுபிடிப்பு துருவம்\n",
      "கண்டுபிடிப்பு தென்\n",
      "கண்டுபிடிப்பு தரையிறங்கு\n",
      "தென் துருவம்\n",
      "தென் தென்\n",
      "தென் தரையிறங்கு\n",
      "வெற்றி துருவம்\n",
      "வெற்றி தென்\n",
      "வெற்றி தரையிறங்கு\n",
      "நாடு துருவம்\n",
      "நாடு தென்\n",
      "நாடு தரையிறங்கு\n",
      "நிலவு துருவம்\n",
      "நிலவு தென்\n",
      "நிலவு தரையிறங்கு\n",
      "துருவம் துருவம்\n",
      "துருவம் தென்\n",
      "துருவம் தரையிறங்கு\n",
      "இந்தியா துருவம்\n",
      "இந்தியா தென்\n",
      "இந்தியா தரையிறங்கு\n",
      "கால்நடை துருவம்\n",
      "கால்நடை தென்\n",
      "கால்நடை தரையிறங்கு\n",
      "திட்டம் துருவம்\n",
      "திட்டம் தென்\n",
      "திட்டம் தரையிறங்கு\n",
      "தென் துருவம்\n",
      "தென் தென்\n",
      "தென் தரையிறங்கு\n",
      "தரையிறங்கு துருவம்\n",
      "தரையிறங்கு தென்\n",
      "தரையிறங்கு தரையிறங்கு\n",
      "வரு துருவம்\n",
      "வரு தென்\n",
      "வரு தரையிறங்கு\n",
      "துருவம் துருவம்\n",
      "துருவம் தென்\n",
      "துருவம் தரையிறங்கு\n",
      "வாரம் துருவம்\n",
      "வாரம் தென்\n",
      "வாரம் தரையிறங்கு\n",
      "[[], [], [], []]\n",
      "[[]]\n"
     ]
    }
   ],
   "source": [
    "sentence_opps = []\n",
    "query_opps = []\n",
    "\n",
    "for j, sentence in enumerate(sentences_without_proper_nouns, 0):\n",
    "    opps = []\n",
    "    for k, sword in enumerate(sentence, 0):\n",
    "        for i, qword in enumerate(query_without_proper_nouns[0], 0):\n",
    "            if(is_opposite(sword, qword)):\n",
    "                print(sword, qword, \"opposites\")\n",
    "                opps.append(sentences_without_proper_nouns[j][k])\n",
    "                query_opps.append(query_without_proper_nouns[0][i])\n",
    "\n",
    "    sentence_opps.append(opps)\n",
    "    \n",
    "query_opps = [list(set(query_opps))]\n",
    "\n",
    "print(sentence_opps)\n",
    "print(query_opps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['துருவம்', 'தென்', 'தரையிறங்கு']]\n",
      "[['துருவம்', 'தரையிறங்கு', 'தென்', 'இந்தியா'], ['நிலவு', 'தென்', 'துருவம்', 'கண்டுபிடிப்பு'], ['தென்', 'வெற்றி', 'நாடு', 'நிலவு', 'துருவம்', 'இந்தியா', 'கால்நடை', 'திட்டம்'], ['துருவம்', 'வாரம்', 'தென்', 'தரையிறங்கு', 'வரு']]\n"
     ]
    }
   ],
   "source": [
    "query_without_opposites = []\n",
    "sentences_without_opposites = []\n",
    "for i in range(len(sentences_without_proper_nouns)):\n",
    "    sentences_without_opposites.append(list(set(sentences_without_proper_nouns[i]) - set(sentence_opps[i])))\n",
    "query_without_opposites.append(list(set(query_without_proper_nouns[0]) - set(query_opps[0])))\n",
    "\n",
    "print(query_without_opposites)\n",
    "print(sentences_without_opposites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['துருவம் தென் தரையிறங்கு']\n",
      "['துருவம் தரையிறங்கு தென் இந்தியா', 'நிலவு தென் துருவம் கண்டுபிடிப்பு', 'தென் வெற்றி நாடு நிலவு துருவம் இந்தியா கால்நடை திட்டம்', 'துருவம் வாரம் தென் தரையிறங்கு வரு']\n",
      "0.9741255\n",
      "0.84239554\n",
      "0.7459006\n",
      "0.76422\n"
     ]
    }
   ],
   "source": [
    "query_without_opposites_merged = [\" \".join(query) for query in query_without_opposites]\n",
    "sentences_without_opposites_merged =  [\" \".join(sentence) for sentence in sentences_without_opposites]\n",
    "\n",
    "print(query_without_opposites_merged)\n",
    "print(sentences_without_opposites_merged)\n",
    "\n",
    "query_without_opposites_vector = get_sentence_vectors(query_without_opposites_merged)\n",
    "sentences_without_opposites_vector = get_sentence_vectors(sentences_without_opposites_merged)\n",
    "\n",
    "without_opposites_scores = []\n",
    "for vector in sentences_without_opposites_vector:\n",
    "    print(score := cosine_similarity(vector, query_without_opposites_vector[0]))\n",
    "    without_opposites_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "opposite_weights = []\n",
    "influencer = 0.1\n",
    "for opps_list in sentence_opps:\n",
    "    if(len(opps_list) == 0):\n",
    "        opposite_weights.append(1)\n",
    "    else:\n",
    "        opposite_weights.append(influencer * len(opps_list))\n",
    "\n",
    "print(opposite_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.5, 1.0, 0.5]\n",
      "[0.9741255, 0.84239554, 0.7459006, 0.76422]\n",
      "[1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(proper_noun_scores)\n",
    "print(without_opposites_scores)\n",
    "print(opposite_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7908302024006844"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authenticity_measure = np.mean([((x + y) / 2) * z for x, y, z in zip(proper_noun_scores, without_opposites_scores, opposite_weights)], axis = -1)\n",
    "authenticity_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  நடிகர் அஜித்குமாருக்கு  பைக்கை பிறந்தநாள் பரிசாக வழங்கிய ஷாலினி                                  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def keep_tamil_unicode(text):\n",
    "    tamil_pattern = re.compile(r'[\\u0B80-\\u0BFF ]')\n",
    "    tamil_text = ''.join(tamil_pattern.findall(text))\n",
    "    return tamil_text\n",
    "\n",
    "input_text = '''#CelebrityClicks | நடிகர் அஜித்குமாருக்கு DUCATI பைக்கை பிறந்தநாள் பரிசாக வழங்கிய ஷாலினி..!\n",
    "                            #SunNews | #HBDAjithKumar | #AjithKumar | #ShaliniAjithkuma'''\n",
    "tamil_only_text = keep_tamil_unicode(input_text)\n",
    "print(tamil_only_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wm_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
